#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Mon Nov 20 17:16:28 2017

Check for statistically significant differences of patterns of partial
correlations of regional rates of volumetric change between pre-HD and healthy
controls. The null distribution is generated by estimating the differences
between those groups after permutation sampling ("messed-up groups" as the
labels are mixed up).

@author: Eduardo Castro
"""

from sklearn.covariance import LedoitWolf
import pandas as pd
from scipy.stats import zscore
import numpy as np
from os.path import join as opj
import sys
import os
sys.path.append(os.path.abspath(opj(os.path.dirname(__file__), '..', '..')))
from utility_code import misc_utility_fncs as muf
from utility_code import resampling_tools as rt


# Estimate precision matrix with pairs of regions as columns (row form)
def precision_scores(vols_df, pcorrs=False):

    prec_scores = pd.DataFrame(index={0})

    # Generate dataframe with pregion indexes
    prec_df = vols_df.cov().copy()

    # l2-norm regularized MLE of the covariance with LedoitWolf method
    lw = LedoitWolf()
    lw.fit(vols_df)
    prec_fitted = lw.precision_
    prec_df[prec_df.columns] = prec_fitted

    # Estimate partial correlations if specified (formula from Wikipedia)
    pcorr_df = prec_df.copy()
    if pcorrs:
        pcorr_df[pcorr_df.columns] = 0
        xv, yv = np.meshgrid(np.diagonal(prec_df), np.diagonal(prec_df))
        pcorr_df[pcorr_df.columns] = -prec_df / np.sqrt(xv * yv)
    else:
        pcorr_df = -prec_df

    # Generate dataframe with precision scores as a dataframe with a single row
    # (to be able to deal with multiple samples)
    list_df = vols_df.columns.tolist()
    nregions = vols_df.shape[1]
    for i1 in range(0, nregions):
        for j1 in range(i1+1, nregions):
            prec_scores[list_df[i1] + '_vs_' + list_df[j1] + '_prec'] = \
                pcorr_df.loc[list_df[i1], list_df[j1]]

    basis_matrix = prec_df.copy()
    basis_matrix[basis_matrix.columns] = 0

    return prec_scores, basis_matrix


"""
INPUT PARAMS
"""

# General setup variables
nbootstrap = int(1e3)  # int(1e4)
nperm = int(1e5)
conf_level = 0.05
pcorrs = True
compar_corr = False     # actual correction for multiple comp. or lenient one

regions = ['Lateral-Ventricle',
           'Brain-Stem',
           'Hippocampus',
           'Amygdala',
           'Accumbens-area',
           'Thalamus-Proper',
           'Caudate',
           'Putamen',
           'Pallidum']

study_id = 'TRACK'
analysis_dir = opj('/data2/eduardo/results/HD_n_Controls',
                   'ROI-feats/slopes_longit/', study_id, 'analysis')
in_csv = opj(analysis_dir, 'vols_slopes_' + study_id + '.csv')
group_controls = 'control'
group_patients = 'preHD'

out_csv = opj(analysis_dir, 'final_precision_for_plot.csv')
prec_perm_fn = opj(analysis_dir, 'precision_permutation_diff.csv')


"""
MAIN CODE
"""

# Retrieve volumentric data, zscore for both groups, retrieve indexes for 2 groups
slopes_df = pd.read_csv(in_csv)
del slopes_df['Unnamed: 0']
slopes_df.reset_index(inplace=True)
del slopes_df['index']

slopes_df[regions] = slopes_df[regions].apply(zscore)
idx_all = slopes_df.index.to_list()
idx_control = slopes_df[slopes_df.group == group_controls].index.to_list()
idx_patient = slopes_df[slopes_df.group == group_patients].index.to_list()
slopes_df = slopes_df[regions]

# Do several permutations of the data, retrieve "group" data for each of them
prec_diff_perm_accum = pd.DataFrame()
for perm_iter in range(nperm):
    if np.mod(perm_iter, 10) == 0:
        print('sample #{0} of {1}'.format(perm_iter, nperm))

    # Retrieve permutation indexes
    perm_idx_all = rt.gen_resample_indexes(idx_all, replace=False,
                                           random_state=perm_iter)
    perm_idx_control = perm_idx_all[:len(idx_control)]
    perm_idx_patient = perm_idx_all[len(idx_control):]

    # Retrieve entries from both groups
    slopes_control_perm = slopes_df.loc[perm_idx_control]
    slopes_patient_perm = slopes_df.loc[perm_idx_patient]

    # Compute precision values for both, subtract and accumulate
    prec_control_perm = precision_scores(slopes_control_perm, pcorrs)[0]
    prec_patient_perm = precision_scores(slopes_patient_perm, pcorrs)[0]
    prec_diff_perm_accum = prec_diff_perm_accum.append(prec_patient_perm -
                                                       prec_control_perm)

prec_diff_perm_accum.reset_index(inplace=True)
del prec_diff_perm_accum['index']
prec_diff_perm_accum.to_csv(prec_perm_fn)

# Retrieve observed estimation of partial correlations for both groups
slopes_control_obs = slopes_df.loc[idx_control]
slopes_patient_obs = slopes_df.loc[idx_patient]
prec_control_obs, basis_matrix = precision_scores(slopes_control_obs, pcorrs)
prec_patient_obs = precision_scores(slopes_patient_obs, pcorrs)[0]
prec_diff_obs = prec_patient_obs - prec_control_obs

# Estimate direction (sign) and size of effects (p-value)
pvalue_df = pd.DataFrame(index={0}, columns=prec_diff_obs.columns.tolist())
sign_df = pd.DataFrame(index={0}, columns=prec_diff_obs.columns.tolist())

for cname in prec_diff_obs.columns.tolist():
    obs_val = prec_diff_obs[cname].values[0]
    perm_distr = prec_diff_perm_accum[cname].values
    frac_sign = rt.gen_conf_interval(perm_distr, obs_val)[0]
    pvalue_df[cname] = np.abs(frac_sign)
    sign_df[cname] = np.sign(frac_sign)

# Correct p-values for multiple comparisons
pvalue_df = pvalue_df.T
if compar_corr:
    pvalue_df['pval_corr'] = muf.multiple_compar_corr(pvalue_df[0], conf_level,
                                                     method='FDR')
else:
    pvalue_df['pval_corr'] = 1.
    pvalue_df.loc[pvalue_df[0] < conf_level,
                  'pval_corr'] = pvalue_df.loc[pvalue_df[0] < conf_level, 0]

# Compute -log(pval) effects; rearrange results as a matrix
pvalue_df['logEffects'] = -np.log10(pvalue_df['pval_corr']) *\
                            np.sign(sign_df).values.ravel()
pvalue_df['index1'] = pvalue_df.index
pvalue_df['index2'] = pvalue_df['index1'].apply(lambda x:
                                                x.split('_vs_')[1].
                                                split('_')[0])
pvalue_df['index1'] = pvalue_df['index1'].apply(lambda x: x.split('_vs_')[0])

final_pvalue_matrix = basis_matrix.copy()
list_df = basis_matrix.columns.tolist()
nregions = len(list_df)
for i1 in range(0, nregions):
    for j1 in range(i1+1, nregions):
        final_pvalue_matrix.loc[list_df[i1], list_df[j1]] = \
            pvalue_df.loc[(pvalue_df['index1'] == list_df[i1]) &
                          (pvalue_df['index2'] == list_df[j1]),
                          'logEffects'].values

# Make matrix symmetric, store
final_pvalue_matrix[final_pvalue_matrix.columns] = final_pvalue_matrix.values\
    + final_pvalue_matrix.values.T
final_pvalue_matrix.to_csv(out_csv)
